{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efce462-17fa-4580-96f0-553257bdbc68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikas/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4131e408-35b8-42c9-8690-3a3ca776898c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.2\n",
      "1.18.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b01a1-5837-47ed-90b2-277f54a8ccda",
   "metadata": {},
   "source": [
    "## Get list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7539cad9-5dda-46f2-9cfa-5bc8e82b64d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dir = \"/home/vikas/Desktop/Globus/gaia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbb5ba6-88df-46d4-910a-78e6be58e7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_files(raw_dir, ext):\n",
    "\n",
    "    files_found = []\n",
    "\n",
    "    for path, dirs, files in os.walk(raw_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(ext):\n",
    "                files_found.append(os.path.join(path, file))\n",
    "\n",
    "    return files_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f66379-5ca9-41cd-80be-8007993d458e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the sorting key function\n",
    "def healpix_sort_key(path):\n",
    "    \n",
    "    # Extract the number after 'healpix=' using string manipulation\n",
    "    healpix_part = path.split('healpix=')[1]\n",
    "    healpix_number = int(healpix_part.split('/')[0])\n",
    "    \n",
    "    return healpix_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6124a49f-7a7d-4713-8b48-0e74e097b9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sorted_files(start, end):\n",
    "    \n",
    "    files_hdf = get_files(raw_dir, \".hdf5\")\n",
    "    \n",
    "    files_hdf_sorted = sorted(files_hdf, key = healpix_sort_key)\n",
    "    \n",
    "    return files_hdf_sorted[start:end]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5521bc-b102-4803-9590-4198eca9f52a",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8d5a8f-445b-47b3-971b-a1dc9fb6b3ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(start, end):\n",
    "    \n",
    "    # Total number of files/shards is 1882    \n",
    "    data_files = {\n",
    "    \"train\": sorted_files(start, end)\n",
    "    }\n",
    "    \n",
    "    hdf_dset = load_dataset(raw_dir,\n",
    "                            split = 'train',\n",
    "                            num_proc = 24,\n",
    "                            streaming = False,\n",
    "                            data_files = data_files)\n",
    "    \n",
    "    #hdf_dset = hdf_dset.with_format('numpy')\n",
    "    \n",
    "    return hdf_dset   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e128905-4cef-435e-bf93-9402f11eadd6",
   "metadata": {},
   "source": [
    "## Convert to Polars DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68cf6bc7-b287-4804-86f5-7ddc7a27f312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_object_id(hdf_dset):\n",
    "    \n",
    "    object_id_all = hdf_dset[:][\"object_id\"]\n",
    "    \n",
    "    object_series = pl.Series(\"object_id\", object_id_all)\n",
    "    \n",
    "    return object_series    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f051a5bc-c642-4a96-94ca-8fb3478e13c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dset_to_polars(hdf_dset, feature, object_series):\n",
    "    \n",
    "    # Collect all rows    \n",
    "    feature_all = hdf_dset[:][feature]\n",
    "    \n",
    "    # Convert to Polars DataFrame    \n",
    "    df_feature_all = pl.DataFrame(feature_all)\n",
    "    \n",
    "    # Add object_id as the first column    \n",
    "    df_feature_all.insert_column(0, object_series)\n",
    "    \n",
    "    return df_feature_all   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c6e9aef-1d01-49e7-a909-94a0027c8068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hdf_dset = load_data(10, 58)\n",
    "\n",
    "# object_series = get_object_id(hdf_dset)\n",
    "\n",
    "# df_feature = dset_to_polars(hdf_dset, \"gspphot\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f6255a-2969-4549-8942-3e408313052e",
   "metadata": {},
   "source": [
    "## Convert to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b338e98-c575-4a84-b8d8-6271cf744000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dest_dir = \"/home/vikas/Desktop/Globus/gaia_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e2c6d3a-3e74-4368-bb22-8aa7e602b5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hdf_to_parquet(start, end):\n",
    "\n",
    "    hdf_dset = load_data(start, end)\n",
    "\n",
    "    object_series = get_object_id(hdf_dset)\n",
    "\n",
    "    features_list = [\n",
    "                     \"photometry\",\n",
    "                     \"astrometry\",\n",
    "                     \"radial_velocity\",\n",
    "                     \"gspphot\"        \n",
    "                    ]\n",
    "    \n",
    "    for feature in features_list:\n",
    "        try:\n",
    "            df_feature = dset_to_polars(hdf_dset, feature, object_series)\n",
    "            \n",
    "            dest_parquet = os.path.join(\n",
    "                                        dest_dir,\n",
    "                                        feature,\n",
    "                                        f\"gaia_{feature}_{start}_{end}.parquet\"\n",
    "                                       )\n",
    "            \n",
    "            df_feature.write_parquet(\n",
    "                dest_parquet,\n",
    "                compression = \"zstd\",\n",
    "                compression_level = 22,\n",
    "                \n",
    "            )\n",
    "            \n",
    "        except:\n",
    "            print(f\"Unable to create parquet output for {feature}!\")\n",
    "            continue\n",
    "            \n",
    "    hdf_dset, object_series = None, None\n",
    "            \n",
    "    return None        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bb188-c8d8-4a21-9471-152ab987c6e5",
   "metadata": {},
   "source": [
    "## Run export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8840473f-2f20-4f8e-9ccf-91a1533e19cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_total_file_size(paths):\n",
    "    \n",
    "    file_sizes = []\n",
    "    \n",
    "    for path in paths:\n",
    "        try:\n",
    "            # Get file size in bytes\n",
    "            size = os.path.getsize(path)  \n",
    "            file_sizes.append(size)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            # Handle case where file does not exist\n",
    "            print(f\"{path} not found!\")\n",
    "            continue\n",
    "            \n",
    "    # Size in GB            \n",
    "    return sum(file_sizes) / (1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "161f0a9c-2aea-416f-9639-e8a2a0b46c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_export(start):\n",
    "    \n",
    "    # Starting at num_proc = 1 as an initial estimate\n",
    "    end = start + 1\n",
    "    cycle_end = False\n",
    "    \n",
    "    while end <= 1882:\n",
    "        \n",
    "        print(f\"File start: {start}\")\n",
    "        print(f\"Initial file end: {end}\")    \n",
    "        \n",
    "        size_on_disk = get_total_file_size(sorted_files(start, end))\n",
    "    \n",
    "        # Keep increasing batch size until total size ~ 12 GB    \n",
    "        while size_on_disk < 1.00 and end < 1881:\n",
    "            \n",
    "            end = end + 1\n",
    "            size_on_disk = get_total_file_size(sorted_files(start, end))       \n",
    "            \n",
    "        print(f\"New file end: {end}\")\n",
    "        \n",
    "        hdf_to_parquet(start, end)\n",
    "        \n",
    "        if cycle_end:\n",
    "            break\n",
    "            \n",
    "        start = end\n",
    "        end = start + 1\n",
    "        \n",
    "        if end > 1882:\n",
    "            end = 1882\n",
    "            cycle_end = True        \n",
    "            \n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c2e05-8999-49b7-8404-5d5c9afe6f97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File start: 466\n",
      "Initial file end: 467\n",
      "New file end: 477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 24 to 11 for the train split as it only contains 11 shards.\n",
      "Generating train split: 43533 examples [00:26, 1746.22 examples/s]"
     ]
    }
   ],
   "source": [
    "run_export(466)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67945dec-d746-4939-8b11-a3a4c26a8e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get_total_file_size(sorted_files(466, 478))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3984da6-de85-42a0-b005-da309cd2041f",
   "metadata": {},
   "source": [
    "## Test exported parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "358f3962-be65-43c0-8cfe-186fade33f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_dir = \"/home/vikas/Desktop/Globus/gaia_parquet/photometry/*.parquet\"\n",
    "\n",
    "# query = (pl\n",
    "#          .scan_parquet(test_dir)\n",
    "#          .filter(pl.col(\"phot_g_mean_flux\") > 200000)\n",
    "#         )\n",
    "\n",
    "# query.collect(streaming=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
